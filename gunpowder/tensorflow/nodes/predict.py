import logging
import numpy as np

from gunpowder.ext import tensorflow as tf
from gunpowder.nodes.generic_predict import GenericPredict
from gunpowder.array import ArrayKey, Array
from gunpowder.tensorflow.local_server import LocalServer

logger = logging.getLogger(__name__)

class Predict(GenericPredict):
    '''Tensorflow implementation of :class:`gunpowder.nodes.Predict`.

    Args:

        model_name: Basename of a tensorflow model storing (minimally) the
            tensorflow graph as model_name.meta and possibly associated tensor metadata as 
            model_name.index and saved tensor values as model_name.data (aka a checkpoint), 
            as created by :class:`gunpowder.nodes.Train`, for example. 

        inputs (dict): Dictionary from the names of input tensors in the
            network to :class:``ArrayKey`` or batch attribute name as string.

        outputs (dict): Dictionary from the names of output tensors in the
            network to :class:``ArrayKey``. New arrays will be generated by
            this node for each entry (if requested downstream).

        array_specs (dict, optional): An optional dictionary of
            :class:`ArrayKey` to :class:`ArraySpec` to set the array specs
            of generated arrays (``outputs``). This is useful to set the
            ``voxel_size``, for example, if they differ from the voxel size of
            the input arrays. Only fields that are not ``None`` in the given
            :class:`ArraySpec` will be used.

        meta_graph: (str, optional): An optional path to a custom meta graph
            that should be used for prediction. The checkpoint associated to
            the model_name is used to restore the values of matching variable
            names in the meta_graph. Note that the computation graph specified
            here can differ from the one associated to model_name.
    '''

    def __init__(
            self,
            model_name,
            inputs,
            outputs,
            array_specs=None,
            meta_graph=None):

        super(Predict, self).__init__(
            inputs,
            outputs,
            array_specs,
            spawn_subprocess=False)
        self.model_name = model_name
        self.session = None
        self.graph = None
        self.meta_graph = meta_graph

    def start(self):

        target = LocalServer.get_target()
        logger.info("Initializing tf session, connecting to %s...", target)

        self.graph = tf.Graph()
        self.session = tf.Session(
            target=target,
            graph=self.graph)

        with self.graph.as_default():
            self.__read_model()

    def predict(self, batch, request):

        logger.debug("predicting in batch %i", batch.id)

        array_outputs = self.__collect_requested_outputs(request)
        inputs = self.__collect_provided_inputs(batch)

        # compute outputs
        outputs = self.session.run(array_outputs, feed_dict=inputs)

        for array_key in array_outputs:
            spec = self.spec[array_key].copy()
            spec.roi = request[array_key].roi
            batch.arrays[array_key] = Array(
                outputs[array_key],
                spec)

        logger.debug("predicted in batch %i", batch.id)

    def stop(self):

        if self.session is not None:
            self.session.close()
            self.graph = None
            self.session = None

    def __read_model(self):

        logger.info("Reading model...")

        # read the meta-graph associated to the model/checkpoint
        if self.meta_graph is None:
            saver = tf.train.import_meta_graph(
                self.model_name + '.meta',
                clear_devices=True)
        # read alternative, custom meta-graph
        else:
            saver = tf.train.import_meta_graph(
                    self.meta_graph,
                    clear_devices=True)

        # restore variables
        saver.restore(self.session, self.model_name)

    def __collect_requested_outputs(self, request):

        array_outputs = {}

        for output_name, array_key in self.outputs.items():
            if array_key in request:
                array_outputs[array_key] = output_name

        return array_outputs

    def __collect_provided_inputs(self, batch):

        inputs = {}

        for input_name, input_key in self.inputs.items():
            if isinstance(input_key, ArrayKey):
                if input_key in batch.arrays:
                    inputs[input_name] = batch.arrays[input_key].data
                else:
                    logger.warn("batch does not contain %s, input %s will not "
                                "be set", input_key, input_name)
            elif isinstance(input_key, np.ndarray):
                inputs[input_name] = input_key
            elif isinstance(input_key, str):
                inputs[input_name] = getattr(batch, input_key)
            else:
                raise Exception(
                    "Unknown network input key {}, can't be given to "
                    "network".format(input_key))

        return inputs
